%//==============================--@--==============================//%
\subsection{1.4 Processos estocásticos (ou aleatórios)}
\label{subsec:stochastic-processes}
%//==============================--@--==============================//%
\subsubsection{1.4.1 Estatísticas}
\paragraph[1.4.1.1 Valor médio]{$\pmb{\star}$ Valor médio}\mbox{}\\
O valor médio de $X(t)$ é definido por
$$
    \mu_x(t) \delequal \mathbb{E}[X(t)] = \int_{\mathbb{R}} x f_{X(t)}(x)\,dx
$$
e é, com generalidade, uma função do tempo.
%//==============================--@--==============================//%
\paragraph[1.4.1.2 Função de autocorrelação]{$\pmb{\star}$ Função de autocorrelação}\mbox{}\\
A função de autocorrelação é definida por
$$
    R_x(t_1, t_2) \delequal \mathbb{E}[X(t_1)X(t_2)] = \int_{\mathbb{R}^2} x_1 x_2\, f_{X(t_1)X(t_2)}(x_1, x_2)\,dx_1\, dx_2
$$
A função de autocorrelação é interpretável como uma medida de semelhança entre as VA's $X(t_1)$ e $X(t_2)$ e é, com generalidade, uma função de $t_1$ e $t_2$. Para $t_2 = t_1$, toma o valor o valor $R_x(t_1,t_1) = \mathbb{E}[X^2(t_1)]$, que é o \textit{valor quadrático médio} de $X(t_1)$.
%//==============================--@--==============================//%
\paragraph[1.4.1.3 Função de autocovariância]{$\pmb{\star}$ Função de autocovariância}\mbox{}\\
$$
    \text{Cov}_x(t_1, t_2) \delequal \mathbb{E}[(X(t_1) - \mu_x(t_1))(X(t_2) - \mu_x(t_2))] = R_x(t_1,t_2) - \mu_x(t_1)\mu_x(t_2)
$$
Para $t_2 = t_1$, Cov$_x(t_1,t_1)$ é a variância de $X(t)$ (i.e., $\sigma_x^{2} \delequal \mathbb{E}[(X^2(t) - \mu_x(t))^2]$).
%//==============================--@--==============================//%
\paragraph[1.4.1.4 Função de correlação cruzada]{$\pmb{\star}$ Função de correlação cruzada}\mbox{}\\
A função de correlação cruzada entre $X(t)$ e $Y(t)$ é dada por
$$
    R_{xy}(t_1,t_2) \delequal \mathbb{E}[X(t_1) Y(t_2)] = \int_{\mathbb{R}^2} x y\, f_{X(t_1)Y(t_2)}(x, y)\,dx\, dy
$$
onde $f_{X(t_1)Y(t_2)}(x,y)$ denota a função densidade de probabilidade conjunta entre as VA $X(t_1)$ e $Y(t_2)$. A função de correlação $R_{xy}(t_1,t_2)$ é interpretável como uma medida de semelhança entre $X(t_1)$ e $Y(t_2)$ e é, com generalidade, uma função de $t_1$ e $t_2$.
%//==============================--@--==============================//%
\subsubsection{1.4.2 Processos estacionários}

``Um processo diz-se \textit{estacionário} se as suas propriedades estatísticas são invariantes a translações no tempo, ou, de forma equivalente, não dependem da origem dos tempos.''\cite{Dias2011}

%//==============================--@--==============================//%
\subsubsection{1.4.3 Processos ergódicos}

``Nos processos processos ergódicos as propriedades de conjunto são iguais a propriedades definidas no tempo e determinadas a partir de qualquer função amostra. A ergodicidade é uma propriedade muito relevante, pois em muitas aplicações temos apenas acesso a um número limitado de funções amostra do processo. Este cenário é frequente, por exemplo, no estudo de sistemas de comunicação.''\cite{Dias2011}

\vspace{0.75em}
\noindent Seja $x(t)$ uma dada função amostra do processo $X(t)$. A média e a correlação temporais de $x(t)$ são definidas, respetivamente, como
$$
    \left< x(t) \right> \delequal \lim_{T \to +\infty} \frac{1}{T} \int_{T} x(t)\, dt\quad \land \quad 
    \left< x(t+\tau)x(t) \right> \delequal \lim_{T \to +\infty} \frac{1}{T} \int_{T} x(t+\tau)x(t)\, dt
$$

\begin{theo}[\underline{Def.:} Processo ergódico \cite{Dias2011}]{def:ergodic}\label{def:ergodic}
    Um processo $X(t)$ diz-se ergódico, se, e somente se, as medidas de conjunto são iguais às médias temporais para qualquer função amostra. Tem-se, nomeadamente, para qualquer função amostra $x(t)$
    $$
        \mu_x = \left< x(t) \right>
    $$
    $$
        R_x(\tau) = \left< x(t+\tau)x(t) \right>
    $$
\end{theo}

\noindent ``Num processo ergódico todas as médias de conjunto são iguais às médias temporais de uma de uma qualquer função amostra. Uma vez que as médias temporais não dependem do tempo tempo, resulta que um processo ergódico é necessariamente um processo estacionário. O inverso não é verdadeiro.''\cite{Dias2011}

%//==============================--@--==============================//%
\subsubsection{1.4.3 Potência de um sinal aleatório e densidade espectral de potência}

A potência do processo $X(t)$ no intervalo $[-T/2, T/2]$ é definida como
$$
    P_T \delequal \frac{1}{T} \int_{T} X^2(t)\, dt
$$
Note-se que $P_T$ é uma VA uma vez que é função das VA's de $X(t)$. Define-se a potência média de $X(t)$ como
$$
    \overline{P} \delequal \lim_{T \to +\infty} \mathbb{E}[P_T] = \lim_{T \to +\infty} \frac{1}{T} \int_{T} \mathbb{E}[X^2(t)]\, dt =
    \left< \mathbb{E}[X^2(t)] \right>
$$
No caso de $X(t)$ ser estacionário temos $\overline{P} = R_x(0)$.

\begin{theo}[\underline{Teorema de Wiener-Khinchin}]{def:wierner-khinchin-e}\label{def:wierner-khinchin-e}
    Num processo $X(t)$ estacionário, a densidade espectral de potência, $S_x(f)$ e a função de densidade de autocorrelação constítuem um par de Fourier:
    \begin{align*}
        S_x(f) &= \mathcal{F}\{R_x(\tau)\} = \int_{\mathbb{R}} R_x(\tau)\, e^{-j2\pi f \tau}\, d\tau \\
        R_x(\tau) &= \mathcal{F}^{-1}\{S_x(f)\} = \int_{\mathbb{R}} \psi_x(f)\, e^{j2\pi f \tau}\, df
    \end{align*}
\end{theo}

%//==============================--@--==============================//%